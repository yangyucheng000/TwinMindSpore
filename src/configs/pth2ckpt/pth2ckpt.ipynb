{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd6e47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore import Tensor, dtype\n",
    "from mindspore import save_checkpoint\n",
    "from mindspore import Parameter\n",
    "model = torch.load(\"alt_gvt_base.pth\", map_location='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3f5729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keys = []\n",
    "nums = 0.\n",
    "for key in model.keys():\n",
    "    if \"num_batches_tracked\" not in key:\n",
    "        model_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "172d1c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.patch_embeds.0.proj.weight\n",
      "model.patch_embeds.0.proj.bias\n",
      "patch_embeds.0.norm.weight\n",
      "patch_embeds.0.norm.bias\n",
      "model.patch_embeds.1.proj.weight\n",
      "model.patch_embeds.1.proj.bias\n",
      "patch_embeds.1.norm.weight\n",
      "patch_embeds.1.norm.bias\n",
      "model.patch_embeds.2.proj.weight\n",
      "model.patch_embeds.2.proj.bias\n",
      "patch_embeds.2.norm.weight\n",
      "patch_embeds.2.norm.bias\n",
      "model.patch_embeds.3.proj.weight\n",
      "model.patch_embeds.3.proj.bias\n",
      "patch_embeds.3.norm.weight\n",
      "patch_embeds.3.norm.bias\n",
      "norm.weight\n",
      "norm.bias\n",
      "model.head.weight\n",
      "model.head.bias\n",
      "model.pos_block.0.proj.0.weight\n",
      "model.pos_block.0.proj.0.bias\n",
      "model.pos_block.1.proj.0.weight\n",
      "model.pos_block.1.proj.0.bias\n",
      "model.pos_block.2.proj.0.weight\n",
      "model.pos_block.2.proj.0.bias\n",
      "model.pos_block.3.proj.0.weight\n",
      "model.pos_block.3.proj.0.bias\n",
      "blocks.0.0.norm1.weight\n",
      "blocks.0.0.norm1.bias\n",
      "blocks.0.0.norm2.weight\n",
      "blocks.0.0.norm2.bias\n",
      "model.blocks.0.0.mlp.fc1.weight\n",
      "model.blocks.0.0.mlp.fc1.bias\n",
      "model.blocks.0.0.mlp.fc2.weight\n",
      "model.blocks.0.0.mlp.fc2.bias\n",
      "blocks.0.0.attn.qkv.weight\n",
      "blocks.0.0.attn.qkv.bias\n",
      "model.blocks.0.0.attn.proj.weight\n",
      "model.blocks.0.0.attn.proj.bias\n",
      "blocks.0.1.norm1.weight\n",
      "blocks.0.1.norm1.bias\n",
      "blocks.0.1.norm2.weight\n",
      "blocks.0.1.norm2.bias\n",
      "model.blocks.0.1.mlp.fc1.weight\n",
      "model.blocks.0.1.mlp.fc1.bias\n",
      "model.blocks.0.1.mlp.fc2.weight\n",
      "model.blocks.0.1.mlp.fc2.bias\n",
      "model.blocks.0.1.attn.q.weight\n",
      "model.blocks.0.1.attn.q.bias\n",
      "blocks.0.1.attn.kv.weight\n",
      "blocks.0.1.attn.kv.bias\n",
      "model.blocks.0.1.attn.proj.weight\n",
      "model.blocks.0.1.attn.proj.bias\n",
      "model.blocks.0.1.attn.sr.weight\n",
      "model.blocks.0.1.attn.sr.bias\n",
      "blocks.0.1.attn.norm.weight\n",
      "blocks.0.1.attn.norm.bias\n",
      "blocks.1.0.norm1.weight\n",
      "blocks.1.0.norm1.bias\n",
      "blocks.1.0.norm2.weight\n",
      "blocks.1.0.norm2.bias\n",
      "model.blocks.1.0.mlp.fc1.weight\n",
      "model.blocks.1.0.mlp.fc1.bias\n",
      "model.blocks.1.0.mlp.fc2.weight\n",
      "model.blocks.1.0.mlp.fc2.bias\n",
      "blocks.1.0.attn.qkv.weight\n",
      "blocks.1.0.attn.qkv.bias\n",
      "model.blocks.1.0.attn.proj.weight\n",
      "model.blocks.1.0.attn.proj.bias\n",
      "blocks.1.1.norm1.weight\n",
      "blocks.1.1.norm1.bias\n",
      "blocks.1.1.norm2.weight\n",
      "blocks.1.1.norm2.bias\n",
      "model.blocks.1.1.mlp.fc1.weight\n",
      "model.blocks.1.1.mlp.fc1.bias\n",
      "model.blocks.1.1.mlp.fc2.weight\n",
      "model.blocks.1.1.mlp.fc2.bias\n",
      "model.blocks.1.1.attn.q.weight\n",
      "model.blocks.1.1.attn.q.bias\n",
      "blocks.1.1.attn.kv.weight\n",
      "blocks.1.1.attn.kv.bias\n",
      "model.blocks.1.1.attn.proj.weight\n",
      "model.blocks.1.1.attn.proj.bias\n",
      "model.blocks.1.1.attn.sr.weight\n",
      "model.blocks.1.1.attn.sr.bias\n",
      "blocks.1.1.attn.norm.weight\n",
      "blocks.1.1.attn.norm.bias\n",
      "blocks.2.0.norm1.weight\n",
      "blocks.2.0.norm1.bias\n",
      "blocks.2.0.norm2.weight\n",
      "blocks.2.0.norm2.bias\n",
      "model.blocks.2.0.mlp.fc1.weight\n",
      "model.blocks.2.0.mlp.fc1.bias\n",
      "model.blocks.2.0.mlp.fc2.weight\n",
      "model.blocks.2.0.mlp.fc2.bias\n",
      "blocks.2.0.attn.qkv.weight\n",
      "blocks.2.0.attn.qkv.bias\n",
      "model.blocks.2.0.attn.proj.weight\n",
      "model.blocks.2.0.attn.proj.bias\n",
      "blocks.2.1.norm1.weight\n",
      "blocks.2.1.norm1.bias\n",
      "blocks.2.1.norm2.weight\n",
      "blocks.2.1.norm2.bias\n",
      "model.blocks.2.1.mlp.fc1.weight\n",
      "model.blocks.2.1.mlp.fc1.bias\n",
      "model.blocks.2.1.mlp.fc2.weight\n",
      "model.blocks.2.1.mlp.fc2.bias\n",
      "model.blocks.2.1.attn.q.weight\n",
      "model.blocks.2.1.attn.q.bias\n",
      "blocks.2.1.attn.kv.weight\n",
      "blocks.2.1.attn.kv.bias\n",
      "model.blocks.2.1.attn.proj.weight\n",
      "model.blocks.2.1.attn.proj.bias\n",
      "model.blocks.2.1.attn.sr.weight\n",
      "model.blocks.2.1.attn.sr.bias\n",
      "blocks.2.1.attn.norm.weight\n",
      "blocks.2.1.attn.norm.bias\n",
      "blocks.2.2.norm1.weight\n",
      "blocks.2.2.norm1.bias\n",
      "blocks.2.2.norm2.weight\n",
      "blocks.2.2.norm2.bias\n",
      "model.blocks.2.2.mlp.fc1.weight\n",
      "model.blocks.2.2.mlp.fc1.bias\n",
      "model.blocks.2.2.mlp.fc2.weight\n",
      "model.blocks.2.2.mlp.fc2.bias\n",
      "blocks.2.2.attn.qkv.weight\n",
      "blocks.2.2.attn.qkv.bias\n",
      "model.blocks.2.2.attn.proj.weight\n",
      "model.blocks.2.2.attn.proj.bias\n",
      "blocks.2.3.norm1.weight\n",
      "blocks.2.3.norm1.bias\n",
      "blocks.2.3.norm2.weight\n",
      "blocks.2.3.norm2.bias\n",
      "model.blocks.2.3.mlp.fc1.weight\n",
      "model.blocks.2.3.mlp.fc1.bias\n",
      "model.blocks.2.3.mlp.fc2.weight\n",
      "model.blocks.2.3.mlp.fc2.bias\n",
      "model.blocks.2.3.attn.q.weight\n",
      "model.blocks.2.3.attn.q.bias\n",
      "blocks.2.3.attn.kv.weight\n",
      "blocks.2.3.attn.kv.bias\n",
      "model.blocks.2.3.attn.proj.weight\n",
      "model.blocks.2.3.attn.proj.bias\n",
      "model.blocks.2.3.attn.sr.weight\n",
      "model.blocks.2.3.attn.sr.bias\n",
      "blocks.2.3.attn.norm.weight\n",
      "blocks.2.3.attn.norm.bias\n",
      "blocks.2.4.norm1.weight\n",
      "blocks.2.4.norm1.bias\n",
      "blocks.2.4.norm2.weight\n",
      "blocks.2.4.norm2.bias\n",
      "model.blocks.2.4.mlp.fc1.weight\n",
      "model.blocks.2.4.mlp.fc1.bias\n",
      "model.blocks.2.4.mlp.fc2.weight\n",
      "model.blocks.2.4.mlp.fc2.bias\n",
      "blocks.2.4.attn.qkv.weight\n",
      "blocks.2.4.attn.qkv.bias\n",
      "model.blocks.2.4.attn.proj.weight\n",
      "model.blocks.2.4.attn.proj.bias\n",
      "blocks.2.5.norm1.weight\n",
      "blocks.2.5.norm1.bias\n",
      "blocks.2.5.norm2.weight\n",
      "blocks.2.5.norm2.bias\n",
      "model.blocks.2.5.mlp.fc1.weight\n",
      "model.blocks.2.5.mlp.fc1.bias\n",
      "model.blocks.2.5.mlp.fc2.weight\n",
      "model.blocks.2.5.mlp.fc2.bias\n",
      "model.blocks.2.5.attn.q.weight\n",
      "model.blocks.2.5.attn.q.bias\n",
      "blocks.2.5.attn.kv.weight\n",
      "blocks.2.5.attn.kv.bias\n",
      "model.blocks.2.5.attn.proj.weight\n",
      "model.blocks.2.5.attn.proj.bias\n",
      "model.blocks.2.5.attn.sr.weight\n",
      "model.blocks.2.5.attn.sr.bias\n",
      "blocks.2.5.attn.norm.weight\n",
      "blocks.2.5.attn.norm.bias\n",
      "blocks.2.6.norm1.weight\n",
      "blocks.2.6.norm1.bias\n",
      "blocks.2.6.norm2.weight\n",
      "blocks.2.6.norm2.bias\n",
      "model.blocks.2.6.mlp.fc1.weight\n",
      "model.blocks.2.6.mlp.fc1.bias\n",
      "model.blocks.2.6.mlp.fc2.weight\n",
      "model.blocks.2.6.mlp.fc2.bias\n",
      "blocks.2.6.attn.qkv.weight\n",
      "blocks.2.6.attn.qkv.bias\n",
      "model.blocks.2.6.attn.proj.weight\n",
      "model.blocks.2.6.attn.proj.bias\n",
      "blocks.2.7.norm1.weight\n",
      "blocks.2.7.norm1.bias\n",
      "blocks.2.7.norm2.weight\n",
      "blocks.2.7.norm2.bias\n",
      "model.blocks.2.7.mlp.fc1.weight\n",
      "model.blocks.2.7.mlp.fc1.bias\n",
      "model.blocks.2.7.mlp.fc2.weight\n",
      "model.blocks.2.7.mlp.fc2.bias\n",
      "model.blocks.2.7.attn.q.weight\n",
      "model.blocks.2.7.attn.q.bias\n",
      "blocks.2.7.attn.kv.weight\n",
      "blocks.2.7.attn.kv.bias\n",
      "model.blocks.2.7.attn.proj.weight\n",
      "model.blocks.2.7.attn.proj.bias\n",
      "model.blocks.2.7.attn.sr.weight\n",
      "model.blocks.2.7.attn.sr.bias\n",
      "blocks.2.7.attn.norm.weight\n",
      "blocks.2.7.attn.norm.bias\n",
      "blocks.2.8.norm1.weight\n",
      "blocks.2.8.norm1.bias\n",
      "blocks.2.8.norm2.weight\n",
      "blocks.2.8.norm2.bias\n",
      "model.blocks.2.8.mlp.fc1.weight\n",
      "model.blocks.2.8.mlp.fc1.bias\n",
      "model.blocks.2.8.mlp.fc2.weight\n",
      "model.blocks.2.8.mlp.fc2.bias\n",
      "blocks.2.8.attn.qkv.weight\n",
      "blocks.2.8.attn.qkv.bias\n",
      "model.blocks.2.8.attn.proj.weight\n",
      "model.blocks.2.8.attn.proj.bias\n",
      "blocks.2.9.norm1.weight\n",
      "blocks.2.9.norm1.bias\n",
      "blocks.2.9.norm2.weight\n",
      "blocks.2.9.norm2.bias\n",
      "model.blocks.2.9.mlp.fc1.weight\n",
      "model.blocks.2.9.mlp.fc1.bias\n",
      "model.blocks.2.9.mlp.fc2.weight\n",
      "model.blocks.2.9.mlp.fc2.bias\n",
      "model.blocks.2.9.attn.q.weight\n",
      "model.blocks.2.9.attn.q.bias\n",
      "blocks.2.9.attn.kv.weight\n",
      "blocks.2.9.attn.kv.bias\n",
      "model.blocks.2.9.attn.proj.weight\n",
      "model.blocks.2.9.attn.proj.bias\n",
      "model.blocks.2.9.attn.sr.weight\n",
      "model.blocks.2.9.attn.sr.bias\n",
      "blocks.2.9.attn.norm.weight\n",
      "blocks.2.9.attn.norm.bias\n",
      "blocks.2.10.norm1.weight\n",
      "blocks.2.10.norm1.bias\n",
      "blocks.2.10.norm2.weight\n",
      "blocks.2.10.norm2.bias\n",
      "model.blocks.2.10.mlp.fc1.weight\n",
      "model.blocks.2.10.mlp.fc1.bias\n",
      "model.blocks.2.10.mlp.fc2.weight\n",
      "model.blocks.2.10.mlp.fc2.bias\n",
      "blocks.2.10.attn.qkv.weight\n",
      "blocks.2.10.attn.qkv.bias\n",
      "model.blocks.2.10.attn.proj.weight\n",
      "model.blocks.2.10.attn.proj.bias\n",
      "blocks.2.11.norm1.weight\n",
      "blocks.2.11.norm1.bias\n",
      "blocks.2.11.norm2.weight\n",
      "blocks.2.11.norm2.bias\n",
      "model.blocks.2.11.mlp.fc1.weight\n",
      "model.blocks.2.11.mlp.fc1.bias\n",
      "model.blocks.2.11.mlp.fc2.weight\n",
      "model.blocks.2.11.mlp.fc2.bias\n",
      "model.blocks.2.11.attn.q.weight\n",
      "model.blocks.2.11.attn.q.bias\n",
      "blocks.2.11.attn.kv.weight\n",
      "blocks.2.11.attn.kv.bias\n",
      "model.blocks.2.11.attn.proj.weight\n",
      "model.blocks.2.11.attn.proj.bias\n",
      "model.blocks.2.11.attn.sr.weight\n",
      "model.blocks.2.11.attn.sr.bias\n",
      "blocks.2.11.attn.norm.weight\n",
      "blocks.2.11.attn.norm.bias\n",
      "blocks.2.12.norm1.weight\n",
      "blocks.2.12.norm1.bias\n",
      "blocks.2.12.norm2.weight\n",
      "blocks.2.12.norm2.bias\n",
      "model.blocks.2.12.mlp.fc1.weight\n",
      "model.blocks.2.12.mlp.fc1.bias\n",
      "model.blocks.2.12.mlp.fc2.weight\n",
      "model.blocks.2.12.mlp.fc2.bias\n",
      "blocks.2.12.attn.qkv.weight\n",
      "blocks.2.12.attn.qkv.bias\n",
      "model.blocks.2.12.attn.proj.weight\n",
      "model.blocks.2.12.attn.proj.bias\n",
      "blocks.2.13.norm1.weight\n",
      "blocks.2.13.norm1.bias\n",
      "blocks.2.13.norm2.weight\n",
      "blocks.2.13.norm2.bias\n",
      "model.blocks.2.13.mlp.fc1.weight\n",
      "model.blocks.2.13.mlp.fc1.bias\n",
      "model.blocks.2.13.mlp.fc2.weight\n",
      "model.blocks.2.13.mlp.fc2.bias\n",
      "model.blocks.2.13.attn.q.weight\n",
      "model.blocks.2.13.attn.q.bias\n",
      "blocks.2.13.attn.kv.weight\n",
      "blocks.2.13.attn.kv.bias\n",
      "model.blocks.2.13.attn.proj.weight\n",
      "model.blocks.2.13.attn.proj.bias\n",
      "model.blocks.2.13.attn.sr.weight\n",
      "model.blocks.2.13.attn.sr.bias\n",
      "blocks.2.13.attn.norm.weight\n",
      "blocks.2.13.attn.norm.bias\n",
      "blocks.2.14.norm1.weight\n",
      "blocks.2.14.norm1.bias\n",
      "blocks.2.14.norm2.weight\n",
      "blocks.2.14.norm2.bias\n",
      "model.blocks.2.14.mlp.fc1.weight\n",
      "model.blocks.2.14.mlp.fc1.bias\n",
      "model.blocks.2.14.mlp.fc2.weight\n",
      "model.blocks.2.14.mlp.fc2.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.2.14.attn.qkv.weight\n",
      "blocks.2.14.attn.qkv.bias\n",
      "model.blocks.2.14.attn.proj.weight\n",
      "model.blocks.2.14.attn.proj.bias\n",
      "blocks.2.15.norm1.weight\n",
      "blocks.2.15.norm1.bias\n",
      "blocks.2.15.norm2.weight\n",
      "blocks.2.15.norm2.bias\n",
      "model.blocks.2.15.mlp.fc1.weight\n",
      "model.blocks.2.15.mlp.fc1.bias\n",
      "model.blocks.2.15.mlp.fc2.weight\n",
      "model.blocks.2.15.mlp.fc2.bias\n",
      "model.blocks.2.15.attn.q.weight\n",
      "model.blocks.2.15.attn.q.bias\n",
      "blocks.2.15.attn.kv.weight\n",
      "blocks.2.15.attn.kv.bias\n",
      "model.blocks.2.15.attn.proj.weight\n",
      "model.blocks.2.15.attn.proj.bias\n",
      "model.blocks.2.15.attn.sr.weight\n",
      "model.blocks.2.15.attn.sr.bias\n",
      "blocks.2.15.attn.norm.weight\n",
      "blocks.2.15.attn.norm.bias\n",
      "blocks.2.16.norm1.weight\n",
      "blocks.2.16.norm1.bias\n",
      "blocks.2.16.norm2.weight\n",
      "blocks.2.16.norm2.bias\n",
      "model.blocks.2.16.mlp.fc1.weight\n",
      "model.blocks.2.16.mlp.fc1.bias\n",
      "model.blocks.2.16.mlp.fc2.weight\n",
      "model.blocks.2.16.mlp.fc2.bias\n",
      "blocks.2.16.attn.qkv.weight\n",
      "blocks.2.16.attn.qkv.bias\n",
      "model.blocks.2.16.attn.proj.weight\n",
      "model.blocks.2.16.attn.proj.bias\n",
      "blocks.2.17.norm1.weight\n",
      "blocks.2.17.norm1.bias\n",
      "blocks.2.17.norm2.weight\n",
      "blocks.2.17.norm2.bias\n",
      "model.blocks.2.17.mlp.fc1.weight\n",
      "model.blocks.2.17.mlp.fc1.bias\n",
      "model.blocks.2.17.mlp.fc2.weight\n",
      "model.blocks.2.17.mlp.fc2.bias\n",
      "model.blocks.2.17.attn.q.weight\n",
      "model.blocks.2.17.attn.q.bias\n",
      "blocks.2.17.attn.kv.weight\n",
      "blocks.2.17.attn.kv.bias\n",
      "model.blocks.2.17.attn.proj.weight\n",
      "model.blocks.2.17.attn.proj.bias\n",
      "model.blocks.2.17.attn.sr.weight\n",
      "model.blocks.2.17.attn.sr.bias\n",
      "blocks.2.17.attn.norm.weight\n",
      "blocks.2.17.attn.norm.bias\n",
      "blocks.3.0.norm1.weight\n",
      "blocks.3.0.norm1.bias\n",
      "blocks.3.0.norm2.weight\n",
      "blocks.3.0.norm2.bias\n",
      "model.blocks.3.0.mlp.fc1.weight\n",
      "model.blocks.3.0.mlp.fc1.bias\n",
      "model.blocks.3.0.mlp.fc2.weight\n",
      "model.blocks.3.0.mlp.fc2.bias\n",
      "blocks.3.0.attn.qkv.weight\n",
      "blocks.3.0.attn.qkv.bias\n",
      "model.blocks.3.0.attn.proj.weight\n",
      "model.blocks.3.0.attn.proj.bias\n",
      "blocks.3.1.norm1.weight\n",
      "blocks.3.1.norm1.bias\n",
      "blocks.3.1.norm2.weight\n",
      "blocks.3.1.norm2.bias\n",
      "model.blocks.3.1.mlp.fc1.weight\n",
      "model.blocks.3.1.mlp.fc1.bias\n",
      "model.blocks.3.1.mlp.fc2.weight\n",
      "model.blocks.3.1.mlp.fc2.bias\n",
      "model.blocks.3.1.attn.q.weight\n",
      "model.blocks.3.1.attn.q.bias\n",
      "blocks.3.1.attn.kv.weight\n",
      "blocks.3.1.attn.kv.bias\n",
      "model.blocks.3.1.attn.proj.weight\n",
      "model.blocks.3.1.attn.proj.bias\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "prefix = \"model.\"\n",
    "for key in model_keys:\n",
    "    name2weight = {}\n",
    "    if \"bn\" in key or \"norm\" in key or \"ln\" in key:\n",
    "        if \"weight\" in key:\n",
    "            name2weight[\"name\"] = prefix + key.replace(\".weight\", \".gamma\")\n",
    "        elif \"bias\" in key:\n",
    "            name2weight[\"name\"] = prefix + key.replace(\".bias\", \".beta\")\n",
    "        elif \"mean\" in key:\n",
    "            name2weight[\"name\"] = prefix + key.replace(\"running_mean\", \"moving_mean\")\n",
    "        elif \"var\" in key:\n",
    "            name2weight[\"name\"] = prefix + key.replace(\"running_var\", \"moving_variance\")\n",
    "        \n",
    "        name2weight[\"data\"] = Parameter(Tensor(model[key].numpy(), dtype.float32),requires_grad=True) \n",
    "        weights.append(name2weight)\n",
    "    elif \"qkv\" in key:\n",
    "        key_q = prefix +  key.replace(\"qkv\", \"q\")\n",
    "        key_k = prefix +  key.replace(\"qkv\", \"k\")\n",
    "        key_v = prefix + key.replace(\"qkv\", \"v\")\n",
    "        shape = model[key].shape[0]//3\n",
    "        weight = Parameter(Tensor(model[key].numpy(), dtype.float32),requires_grad=True) \n",
    "        weight_q = weight[:shape]\n",
    "        weight_k = weight[shape:shape*2]\n",
    "        weight_v = weight[shape*2:]\n",
    "        weights.append({\"name\":key_q, \"data\": weight_q})\n",
    "        weights.append({\"name\":key_k, \"data\": weight_k})\n",
    "        weights.append({\"name\":key_v, \"data\": weight_v})\n",
    "    elif \"kv\" in key:\n",
    "        key_k = prefix +  key.replace(\"kv\", \"k\")\n",
    "        key_v = prefix +  key.replace(\"kv\", \"v\")\n",
    "        shape = model[key].shape[0]//2\n",
    "        weight = Parameter(Tensor(model[key].numpy(), dtype.float32),requires_grad=True) \n",
    "        weight_k = weight[:shape]\n",
    "        weight_v = weight[shape:shape*2]\n",
    "        weights.append({\"name\":key_k, \"data\": weight_k})\n",
    "        weights.append({\"name\":key_v, \"data\": weight_v})\n",
    "    else:\n",
    "        dd = dtype.float32\n",
    "        if \"relative_position_index\" in key:\n",
    "            dd = dtype.int32\n",
    "        weight = Parameter(Tensor(model[key].numpy(), dd),requires_grad=True)\n",
    "        key = prefix + key\n",
    "        weights.append({\"name\": key, \"data\": weight})\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a079bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.patch_embeds.0.proj.weight (96, 3, 4, 4)\n",
      "model.patch_embeds.0.proj.bias (96,)\n",
      "model.patch_embeds.0.norm.gamma (96,)\n",
      "model.patch_embeds.0.norm.beta (96,)\n",
      "model.patch_embeds.1.proj.weight (192, 96, 2, 2)\n",
      "model.patch_embeds.1.proj.bias (192,)\n",
      "model.patch_embeds.1.norm.gamma (192,)\n",
      "model.patch_embeds.1.norm.beta (192,)\n",
      "model.patch_embeds.2.proj.weight (384, 192, 2, 2)\n",
      "model.patch_embeds.2.proj.bias (384,)\n",
      "model.patch_embeds.2.norm.gamma (384,)\n",
      "model.patch_embeds.2.norm.beta (384,)\n",
      "model.patch_embeds.3.proj.weight (768, 384, 2, 2)\n",
      "model.patch_embeds.3.proj.bias (768,)\n",
      "model.patch_embeds.3.norm.gamma (768,)\n",
      "model.patch_embeds.3.norm.beta (768,)\n",
      "model.norm.gamma (768,)\n",
      "model.norm.beta (768,)\n",
      "model.head.weight (1000, 768)\n",
      "model.head.bias (1000,)\n",
      "model.pos_block.0.proj.0.weight (96, 1, 3, 3)\n",
      "model.pos_block.0.proj.0.bias (96,)\n",
      "model.pos_block.1.proj.0.weight (192, 1, 3, 3)\n",
      "model.pos_block.1.proj.0.bias (192,)\n",
      "model.pos_block.2.proj.0.weight (384, 1, 3, 3)\n",
      "model.pos_block.2.proj.0.bias (384,)\n",
      "model.pos_block.3.proj.0.weight (768, 1, 3, 3)\n",
      "model.pos_block.3.proj.0.bias (768,)\n",
      "model.blocks.0.0.norm1.gamma (96,)\n",
      "model.blocks.0.0.norm1.beta (96,)\n",
      "model.blocks.0.0.norm2.gamma (96,)\n",
      "model.blocks.0.0.norm2.beta (96,)\n",
      "model.blocks.0.0.mlp.fc1.weight (384, 96)\n",
      "model.blocks.0.0.mlp.fc1.bias (384,)\n",
      "model.blocks.0.0.mlp.fc2.weight (96, 384)\n",
      "model.blocks.0.0.mlp.fc2.bias (96,)\n",
      "model.blocks.0.0.attn.q.weight (96, 96)\n",
      "model.blocks.0.0.attn.k.weight (96, 96)\n",
      "model.blocks.0.0.attn.v.weight (96, 96)\n",
      "model.blocks.0.0.attn.q.bias (96,)\n",
      "model.blocks.0.0.attn.k.bias (96,)\n",
      "model.blocks.0.0.attn.v.bias (96,)\n",
      "model.blocks.0.0.attn.proj.weight (96, 96)\n",
      "model.blocks.0.0.attn.proj.bias (96,)\n",
      "model.blocks.0.1.norm1.gamma (96,)\n",
      "model.blocks.0.1.norm1.beta (96,)\n",
      "model.blocks.0.1.norm2.gamma (96,)\n",
      "model.blocks.0.1.norm2.beta (96,)\n",
      "model.blocks.0.1.mlp.fc1.weight (384, 96)\n",
      "model.blocks.0.1.mlp.fc1.bias (384,)\n",
      "model.blocks.0.1.mlp.fc2.weight (96, 384)\n",
      "model.blocks.0.1.mlp.fc2.bias (96,)\n",
      "model.blocks.0.1.attn.q.weight (96, 96)\n",
      "model.blocks.0.1.attn.q.bias (96,)\n",
      "model.blocks.0.1.attn.k.weight (96, 96)\n",
      "model.blocks.0.1.attn.v.weight (96, 96)\n",
      "model.blocks.0.1.attn.k.bias (96,)\n",
      "model.blocks.0.1.attn.v.bias (96,)\n",
      "model.blocks.0.1.attn.proj.weight (96, 96)\n",
      "model.blocks.0.1.attn.proj.bias (96,)\n",
      "model.blocks.0.1.attn.sr.weight (96, 96, 8, 8)\n",
      "model.blocks.0.1.attn.sr.bias (96,)\n",
      "model.blocks.0.1.attn.norm.gamma (96,)\n",
      "model.blocks.0.1.attn.norm.beta (96,)\n",
      "model.blocks.1.0.norm1.gamma (192,)\n",
      "model.blocks.1.0.norm1.beta (192,)\n",
      "model.blocks.1.0.norm2.gamma (192,)\n",
      "model.blocks.1.0.norm2.beta (192,)\n",
      "model.blocks.1.0.mlp.fc1.weight (768, 192)\n",
      "model.blocks.1.0.mlp.fc1.bias (768,)\n",
      "model.blocks.1.0.mlp.fc2.weight (192, 768)\n",
      "model.blocks.1.0.mlp.fc2.bias (192,)\n",
      "model.blocks.1.0.attn.q.weight (192, 192)\n",
      "model.blocks.1.0.attn.k.weight (192, 192)\n",
      "model.blocks.1.0.attn.v.weight (192, 192)\n",
      "model.blocks.1.0.attn.q.bias (192,)\n",
      "model.blocks.1.0.attn.k.bias (192,)\n",
      "model.blocks.1.0.attn.v.bias (192,)\n",
      "model.blocks.1.0.attn.proj.weight (192, 192)\n",
      "model.blocks.1.0.attn.proj.bias (192,)\n",
      "model.blocks.1.1.norm1.gamma (192,)\n",
      "model.blocks.1.1.norm1.beta (192,)\n",
      "model.blocks.1.1.norm2.gamma (192,)\n",
      "model.blocks.1.1.norm2.beta (192,)\n",
      "model.blocks.1.1.mlp.fc1.weight (768, 192)\n",
      "model.blocks.1.1.mlp.fc1.bias (768,)\n",
      "model.blocks.1.1.mlp.fc2.weight (192, 768)\n",
      "model.blocks.1.1.mlp.fc2.bias (192,)\n",
      "model.blocks.1.1.attn.q.weight (192, 192)\n",
      "model.blocks.1.1.attn.q.bias (192,)\n",
      "model.blocks.1.1.attn.k.weight (192, 192)\n",
      "model.blocks.1.1.attn.v.weight (192, 192)\n",
      "model.blocks.1.1.attn.k.bias (192,)\n",
      "model.blocks.1.1.attn.v.bias (192,)\n",
      "model.blocks.1.1.attn.proj.weight (192, 192)\n",
      "model.blocks.1.1.attn.proj.bias (192,)\n",
      "model.blocks.1.1.attn.sr.weight (192, 192, 4, 4)\n",
      "model.blocks.1.1.attn.sr.bias (192,)\n",
      "model.blocks.1.1.attn.norm.gamma (192,)\n",
      "model.blocks.1.1.attn.norm.beta (192,)\n",
      "model.blocks.2.0.norm1.gamma (384,)\n",
      "model.blocks.2.0.norm1.beta (384,)\n",
      "model.blocks.2.0.norm2.gamma (384,)\n",
      "model.blocks.2.0.norm2.beta (384,)\n",
      "model.blocks.2.0.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.0.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.0.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.0.mlp.fc2.bias (384,)\n",
      "model.blocks.2.0.attn.q.weight (384, 384)\n",
      "model.blocks.2.0.attn.k.weight (384, 384)\n",
      "model.blocks.2.0.attn.v.weight (384, 384)\n",
      "model.blocks.2.0.attn.q.bias (384,)\n",
      "model.blocks.2.0.attn.k.bias (384,)\n",
      "model.blocks.2.0.attn.v.bias (384,)\n",
      "model.blocks.2.0.attn.proj.weight (384, 384)\n",
      "model.blocks.2.0.attn.proj.bias (384,)\n",
      "model.blocks.2.1.norm1.gamma (384,)\n",
      "model.blocks.2.1.norm1.beta (384,)\n",
      "model.blocks.2.1.norm2.gamma (384,)\n",
      "model.blocks.2.1.norm2.beta (384,)\n",
      "model.blocks.2.1.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.1.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.1.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.1.mlp.fc2.bias (384,)\n",
      "model.blocks.2.1.attn.q.weight (384, 384)\n",
      "model.blocks.2.1.attn.q.bias (384,)\n",
      "model.blocks.2.1.attn.k.weight (384, 384)\n",
      "model.blocks.2.1.attn.v.weight (384, 384)\n",
      "model.blocks.2.1.attn.k.bias (384,)\n",
      "model.blocks.2.1.attn.v.bias (384,)\n",
      "model.blocks.2.1.attn.proj.weight (384, 384)\n",
      "model.blocks.2.1.attn.proj.bias (384,)\n",
      "model.blocks.2.1.attn.sr.weight (384, 384, 2, 2)\n",
      "model.blocks.2.1.attn.sr.bias (384,)\n",
      "model.blocks.2.1.attn.norm.gamma (384,)\n",
      "model.blocks.2.1.attn.norm.beta (384,)\n",
      "model.blocks.2.2.norm1.gamma (384,)\n",
      "model.blocks.2.2.norm1.beta (384,)\n",
      "model.blocks.2.2.norm2.gamma (384,)\n",
      "model.blocks.2.2.norm2.beta (384,)\n",
      "model.blocks.2.2.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.2.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.2.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.2.mlp.fc2.bias (384,)\n",
      "model.blocks.2.2.attn.q.weight (384, 384)\n",
      "model.blocks.2.2.attn.k.weight (384, 384)\n",
      "model.blocks.2.2.attn.v.weight (384, 384)\n",
      "model.blocks.2.2.attn.q.bias (384,)\n",
      "model.blocks.2.2.attn.k.bias (384,)\n",
      "model.blocks.2.2.attn.v.bias (384,)\n",
      "model.blocks.2.2.attn.proj.weight (384, 384)\n",
      "model.blocks.2.2.attn.proj.bias (384,)\n",
      "model.blocks.2.3.norm1.gamma (384,)\n",
      "model.blocks.2.3.norm1.beta (384,)\n",
      "model.blocks.2.3.norm2.gamma (384,)\n",
      "model.blocks.2.3.norm2.beta (384,)\n",
      "model.blocks.2.3.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.3.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.3.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.3.mlp.fc2.bias (384,)\n",
      "model.blocks.2.3.attn.q.weight (384, 384)\n",
      "model.blocks.2.3.attn.q.bias (384,)\n",
      "model.blocks.2.3.attn.k.weight (384, 384)\n",
      "model.blocks.2.3.attn.v.weight (384, 384)\n",
      "model.blocks.2.3.attn.k.bias (384,)\n",
      "model.blocks.2.3.attn.v.bias (384,)\n",
      "model.blocks.2.3.attn.proj.weight (384, 384)\n",
      "model.blocks.2.3.attn.proj.bias (384,)\n",
      "model.blocks.2.3.attn.sr.weight (384, 384, 2, 2)\n",
      "model.blocks.2.3.attn.sr.bias (384,)\n",
      "model.blocks.2.3.attn.norm.gamma (384,)\n",
      "model.blocks.2.3.attn.norm.beta (384,)\n",
      "model.blocks.2.4.norm1.gamma (384,)\n",
      "model.blocks.2.4.norm1.beta (384,)\n",
      "model.blocks.2.4.norm2.gamma (384,)\n",
      "model.blocks.2.4.norm2.beta (384,)\n",
      "model.blocks.2.4.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.4.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.4.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.4.mlp.fc2.bias (384,)\n",
      "model.blocks.2.4.attn.q.weight (384, 384)\n",
      "model.blocks.2.4.attn.k.weight (384, 384)\n",
      "model.blocks.2.4.attn.v.weight (384, 384)\n",
      "model.blocks.2.4.attn.q.bias (384,)\n",
      "model.blocks.2.4.attn.k.bias (384,)\n",
      "model.blocks.2.4.attn.v.bias (384,)\n",
      "model.blocks.2.4.attn.proj.weight (384, 384)\n",
      "model.blocks.2.4.attn.proj.bias (384,)\n",
      "model.blocks.2.5.norm1.gamma (384,)\n",
      "model.blocks.2.5.norm1.beta (384,)\n",
      "model.blocks.2.5.norm2.gamma (384,)\n",
      "model.blocks.2.5.norm2.beta (384,)\n",
      "model.blocks.2.5.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.5.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.5.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.5.mlp.fc2.bias (384,)\n",
      "model.blocks.2.5.attn.q.weight (384, 384)\n",
      "model.blocks.2.5.attn.q.bias (384,)\n",
      "model.blocks.2.5.attn.k.weight (384, 384)\n",
      "model.blocks.2.5.attn.v.weight (384, 384)\n",
      "model.blocks.2.5.attn.k.bias (384,)\n",
      "model.blocks.2.5.attn.v.bias (384,)\n",
      "model.blocks.2.5.attn.proj.weight (384, 384)\n",
      "model.blocks.2.5.attn.proj.bias (384,)\n",
      "model.blocks.2.5.attn.sr.weight (384, 384, 2, 2)\n",
      "model.blocks.2.5.attn.sr.bias (384,)\n",
      "model.blocks.2.5.attn.norm.gamma (384,)\n",
      "model.blocks.2.5.attn.norm.beta (384,)\n",
      "model.blocks.2.6.norm1.gamma (384,)\n",
      "model.blocks.2.6.norm1.beta (384,)\n",
      "model.blocks.2.6.norm2.gamma (384,)\n",
      "model.blocks.2.6.norm2.beta (384,)\n",
      "model.blocks.2.6.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.6.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.6.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.6.mlp.fc2.bias (384,)\n",
      "model.blocks.2.6.attn.q.weight (384, 384)\n",
      "model.blocks.2.6.attn.k.weight (384, 384)\n",
      "model.blocks.2.6.attn.v.weight (384, 384)\n",
      "model.blocks.2.6.attn.q.bias (384,)\n",
      "model.blocks.2.6.attn.k.bias (384,)\n",
      "model.blocks.2.6.attn.v.bias (384,)\n",
      "model.blocks.2.6.attn.proj.weight (384, 384)\n",
      "model.blocks.2.6.attn.proj.bias (384,)\n",
      "model.blocks.2.7.norm1.gamma (384,)\n",
      "model.blocks.2.7.norm1.beta (384,)\n",
      "model.blocks.2.7.norm2.gamma (384,)\n",
      "model.blocks.2.7.norm2.beta (384,)\n",
      "model.blocks.2.7.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.7.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.7.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.7.mlp.fc2.bias (384,)\n",
      "model.blocks.2.7.attn.q.weight (384, 384)\n",
      "model.blocks.2.7.attn.q.bias (384,)\n",
      "model.blocks.2.7.attn.k.weight (384, 384)\n",
      "model.blocks.2.7.attn.v.weight (384, 384)\n",
      "model.blocks.2.7.attn.k.bias (384,)\n",
      "model.blocks.2.7.attn.v.bias (384,)\n",
      "model.blocks.2.7.attn.proj.weight (384, 384)\n",
      "model.blocks.2.7.attn.proj.bias (384,)\n",
      "model.blocks.2.7.attn.sr.weight (384, 384, 2, 2)\n",
      "model.blocks.2.7.attn.sr.bias (384,)\n",
      "model.blocks.2.7.attn.norm.gamma (384,)\n",
      "model.blocks.2.7.attn.norm.beta (384,)\n",
      "model.blocks.2.8.norm1.gamma (384,)\n",
      "model.blocks.2.8.norm1.beta (384,)\n",
      "model.blocks.2.8.norm2.gamma (384,)\n",
      "model.blocks.2.8.norm2.beta (384,)\n",
      "model.blocks.2.8.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.8.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.8.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.8.mlp.fc2.bias (384,)\n",
      "model.blocks.2.8.attn.q.weight (384, 384)\n",
      "model.blocks.2.8.attn.k.weight (384, 384)\n",
      "model.blocks.2.8.attn.v.weight (384, 384)\n",
      "model.blocks.2.8.attn.q.bias (384,)\n",
      "model.blocks.2.8.attn.k.bias (384,)\n",
      "model.blocks.2.8.attn.v.bias (384,)\n",
      "model.blocks.2.8.attn.proj.weight (384, 384)\n",
      "model.blocks.2.8.attn.proj.bias (384,)\n",
      "model.blocks.2.9.norm1.gamma (384,)\n",
      "model.blocks.2.9.norm1.beta (384,)\n",
      "model.blocks.2.9.norm2.gamma (384,)\n",
      "model.blocks.2.9.norm2.beta (384,)\n",
      "model.blocks.2.9.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.9.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.9.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.9.mlp.fc2.bias (384,)\n",
      "model.blocks.2.9.attn.q.weight (384, 384)\n",
      "model.blocks.2.9.attn.q.bias (384,)\n",
      "model.blocks.2.9.attn.k.weight (384, 384)\n",
      "model.blocks.2.9.attn.v.weight (384, 384)\n",
      "model.blocks.2.9.attn.k.bias (384,)\n",
      "model.blocks.2.9.attn.v.bias (384,)\n",
      "model.blocks.2.9.attn.proj.weight (384, 384)\n",
      "model.blocks.2.9.attn.proj.bias (384,)\n",
      "model.blocks.2.9.attn.sr.weight (384, 384, 2, 2)\n",
      "model.blocks.2.9.attn.sr.bias (384,)\n",
      "model.blocks.2.9.attn.norm.gamma (384,)\n",
      "model.blocks.2.9.attn.norm.beta (384,)\n",
      "model.blocks.2.10.norm1.gamma (384,)\n",
      "model.blocks.2.10.norm1.beta (384,)\n",
      "model.blocks.2.10.norm2.gamma (384,)\n",
      "model.blocks.2.10.norm2.beta (384,)\n",
      "model.blocks.2.10.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.10.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.10.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.10.mlp.fc2.bias (384,)\n",
      "model.blocks.2.10.attn.q.weight (384, 384)\n",
      "model.blocks.2.10.attn.k.weight (384, 384)\n",
      "model.blocks.2.10.attn.v.weight (384, 384)\n",
      "model.blocks.2.10.attn.q.bias (384,)\n",
      "model.blocks.2.10.attn.k.bias (384,)\n",
      "model.blocks.2.10.attn.v.bias (384,)\n",
      "model.blocks.2.10.attn.proj.weight (384, 384)\n",
      "model.blocks.2.10.attn.proj.bias (384,)\n",
      "model.blocks.2.11.norm1.gamma (384,)\n",
      "model.blocks.2.11.norm1.beta (384,)\n",
      "model.blocks.2.11.norm2.gamma (384,)\n",
      "model.blocks.2.11.norm2.beta (384,)\n",
      "model.blocks.2.11.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.11.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.11.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.11.mlp.fc2.bias (384,)\n",
      "model.blocks.2.11.attn.q.weight (384, 384)\n",
      "model.blocks.2.11.attn.q.bias (384,)\n",
      "model.blocks.2.11.attn.k.weight (384, 384)\n",
      "model.blocks.2.11.attn.v.weight (384, 384)\n",
      "model.blocks.2.11.attn.k.bias (384,)\n",
      "model.blocks.2.11.attn.v.bias (384,)\n",
      "model.blocks.2.11.attn.proj.weight (384, 384)\n",
      "model.blocks.2.11.attn.proj.bias (384,)\n",
      "model.blocks.2.11.attn.sr.weight (384, 384, 2, 2)\n",
      "model.blocks.2.11.attn.sr.bias (384,)\n",
      "model.blocks.2.11.attn.norm.gamma (384,)\n",
      "model.blocks.2.11.attn.norm.beta (384,)\n",
      "model.blocks.2.12.norm1.gamma (384,)\n",
      "model.blocks.2.12.norm1.beta (384,)\n",
      "model.blocks.2.12.norm2.gamma (384,)\n",
      "model.blocks.2.12.norm2.beta (384,)\n",
      "model.blocks.2.12.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.12.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.12.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.12.mlp.fc2.bias (384,)\n",
      "model.blocks.2.12.attn.q.weight (384, 384)\n",
      "model.blocks.2.12.attn.k.weight (384, 384)\n",
      "model.blocks.2.12.attn.v.weight (384, 384)\n",
      "model.blocks.2.12.attn.q.bias (384,)\n",
      "model.blocks.2.12.attn.k.bias (384,)\n",
      "model.blocks.2.12.attn.v.bias (384,)\n",
      "model.blocks.2.12.attn.proj.weight (384, 384)\n",
      "model.blocks.2.12.attn.proj.bias (384,)\n",
      "model.blocks.2.13.norm1.gamma (384,)\n",
      "model.blocks.2.13.norm1.beta (384,)\n",
      "model.blocks.2.13.norm2.gamma (384,)\n",
      "model.blocks.2.13.norm2.beta (384,)\n",
      "model.blocks.2.13.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.13.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.13.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.13.mlp.fc2.bias (384,)\n",
      "model.blocks.2.13.attn.q.weight (384, 384)\n",
      "model.blocks.2.13.attn.q.bias (384,)\n",
      "model.blocks.2.13.attn.k.weight (384, 384)\n",
      "model.blocks.2.13.attn.v.weight (384, 384)\n",
      "model.blocks.2.13.attn.k.bias (384,)\n",
      "model.blocks.2.13.attn.v.bias (384,)\n",
      "model.blocks.2.13.attn.proj.weight (384, 384)\n",
      "model.blocks.2.13.attn.proj.bias (384,)\n",
      "model.blocks.2.13.attn.sr.weight (384, 384, 2, 2)\n",
      "model.blocks.2.13.attn.sr.bias (384,)\n",
      "model.blocks.2.13.attn.norm.gamma (384,)\n",
      "model.blocks.2.13.attn.norm.beta (384,)\n",
      "model.blocks.2.14.norm1.gamma (384,)\n",
      "model.blocks.2.14.norm1.beta (384,)\n",
      "model.blocks.2.14.norm2.gamma (384,)\n",
      "model.blocks.2.14.norm2.beta (384,)\n",
      "model.blocks.2.14.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.14.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.14.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.14.mlp.fc2.bias (384,)\n",
      "model.blocks.2.14.attn.q.weight (384, 384)\n",
      "model.blocks.2.14.attn.k.weight (384, 384)\n",
      "model.blocks.2.14.attn.v.weight (384, 384)\n",
      "model.blocks.2.14.attn.q.bias (384,)\n",
      "model.blocks.2.14.attn.k.bias (384,)\n",
      "model.blocks.2.14.attn.v.bias (384,)\n",
      "model.blocks.2.14.attn.proj.weight (384, 384)\n",
      "model.blocks.2.14.attn.proj.bias (384,)\n",
      "model.blocks.2.15.norm1.gamma (384,)\n",
      "model.blocks.2.15.norm1.beta (384,)\n",
      "model.blocks.2.15.norm2.gamma (384,)\n",
      "model.blocks.2.15.norm2.beta (384,)\n",
      "model.blocks.2.15.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.15.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.15.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.15.mlp.fc2.bias (384,)\n",
      "model.blocks.2.15.attn.q.weight (384, 384)\n",
      "model.blocks.2.15.attn.q.bias (384,)\n",
      "model.blocks.2.15.attn.k.weight (384, 384)\n",
      "model.blocks.2.15.attn.v.weight (384, 384)\n",
      "model.blocks.2.15.attn.k.bias (384,)\n",
      "model.blocks.2.15.attn.v.bias (384,)\n",
      "model.blocks.2.15.attn.proj.weight (384, 384)\n",
      "model.blocks.2.15.attn.proj.bias (384,)\n",
      "model.blocks.2.15.attn.sr.weight (384, 384, 2, 2)\n",
      "model.blocks.2.15.attn.sr.bias (384,)\n",
      "model.blocks.2.15.attn.norm.gamma (384,)\n",
      "model.blocks.2.15.attn.norm.beta (384,)\n",
      "model.blocks.2.16.norm1.gamma (384,)\n",
      "model.blocks.2.16.norm1.beta (384,)\n",
      "model.blocks.2.16.norm2.gamma (384,)\n",
      "model.blocks.2.16.norm2.beta (384,)\n",
      "model.blocks.2.16.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.16.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.16.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.16.mlp.fc2.bias (384,)\n",
      "model.blocks.2.16.attn.q.weight (384, 384)\n",
      "model.blocks.2.16.attn.k.weight (384, 384)\n",
      "model.blocks.2.16.attn.v.weight (384, 384)\n",
      "model.blocks.2.16.attn.q.bias (384,)\n",
      "model.blocks.2.16.attn.k.bias (384,)\n",
      "model.blocks.2.16.attn.v.bias (384,)\n",
      "model.blocks.2.16.attn.proj.weight (384, 384)\n",
      "model.blocks.2.16.attn.proj.bias (384,)\n",
      "model.blocks.2.17.norm1.gamma (384,)\n",
      "model.blocks.2.17.norm1.beta (384,)\n",
      "model.blocks.2.17.norm2.gamma (384,)\n",
      "model.blocks.2.17.norm2.beta (384,)\n",
      "model.blocks.2.17.mlp.fc1.weight (1536, 384)\n",
      "model.blocks.2.17.mlp.fc1.bias (1536,)\n",
      "model.blocks.2.17.mlp.fc2.weight (384, 1536)\n",
      "model.blocks.2.17.mlp.fc2.bias (384,)\n",
      "model.blocks.2.17.attn.q.weight (384, 384)\n",
      "model.blocks.2.17.attn.q.bias (384,)\n",
      "model.blocks.2.17.attn.k.weight (384, 384)\n",
      "model.blocks.2.17.attn.v.weight (384, 384)\n",
      "model.blocks.2.17.attn.k.bias (384,)\n",
      "model.blocks.2.17.attn.v.bias (384,)\n",
      "model.blocks.2.17.attn.proj.weight (384, 384)\n",
      "model.blocks.2.17.attn.proj.bias (384,)\n",
      "model.blocks.2.17.attn.sr.weight (384, 384, 2, 2)\n",
      "model.blocks.2.17.attn.sr.bias (384,)\n",
      "model.blocks.2.17.attn.norm.gamma (384,)\n",
      "model.blocks.2.17.attn.norm.beta (384,)\n",
      "model.blocks.3.0.norm1.gamma (768,)\n",
      "model.blocks.3.0.norm1.beta (768,)\n",
      "model.blocks.3.0.norm2.gamma (768,)\n",
      "model.blocks.3.0.norm2.beta (768,)\n",
      "model.blocks.3.0.mlp.fc1.weight (3072, 768)\n",
      "model.blocks.3.0.mlp.fc1.bias (3072,)\n",
      "model.blocks.3.0.mlp.fc2.weight (768, 3072)\n",
      "model.blocks.3.0.mlp.fc2.bias (768,)\n",
      "model.blocks.3.0.attn.q.weight (768, 768)\n",
      "model.blocks.3.0.attn.k.weight (768, 768)\n",
      "model.blocks.3.0.attn.v.weight (768, 768)\n",
      "model.blocks.3.0.attn.q.bias (768,)\n",
      "model.blocks.3.0.attn.k.bias (768,)\n",
      "model.blocks.3.0.attn.v.bias (768,)\n",
      "model.blocks.3.0.attn.proj.weight (768, 768)\n",
      "model.blocks.3.0.attn.proj.bias (768,)\n",
      "model.blocks.3.1.norm1.gamma (768,)\n",
      "model.blocks.3.1.norm1.beta (768,)\n",
      "model.blocks.3.1.norm2.gamma (768,)\n",
      "model.blocks.3.1.norm2.beta (768,)\n",
      "model.blocks.3.1.mlp.fc1.weight (3072, 768)\n",
      "model.blocks.3.1.mlp.fc1.bias (3072,)\n",
      "model.blocks.3.1.mlp.fc2.weight (768, 3072)\n",
      "model.blocks.3.1.mlp.fc2.bias (768,)\n",
      "model.blocks.3.1.attn.q.weight (768, 768)\n",
      "model.blocks.3.1.attn.q.bias (768,)\n",
      "model.blocks.3.1.attn.k.weight (768, 768)\n",
      "model.blocks.3.1.attn.v.weight (768, 768)\n",
      "model.blocks.3.1.attn.k.bias (768,)\n",
      "model.blocks.3.1.attn.v.bias (768,)\n",
      "model.blocks.3.1.attn.proj.weight (768, 768)\n",
      "model.blocks.3.1.attn.proj.bias (768,)\n"
     ]
    }
   ],
   "source": [
    "weights_out = []\n",
    "for weight in weights:\n",
    "    weight_out = {}\n",
    "    name = weight[\"name\"]\n",
    "    weight_out[\"name\"] = name\n",
    "    weight_out[\"data\"] = weight[\"data\"]\n",
    "    weights_out.append(weight_out)\n",
    "    print(weight_out[\"name\"], weight_out['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58e81d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(weights_out, \"alt_gvt_base.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7375f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584be6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
